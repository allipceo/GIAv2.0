#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ API ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
ì‘ì„±ì¼: 2025ë…„ 1ì›” 12ì¼
ì‘ì„±ì: ì„œëŒ€ë¦¬ (Lead Developer)
ëª©ì : ì¡°ëŒ€í‘œë‹˜ ê´€ì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° LLM ì—°ë™

ì£¼ìš” ê°œì„ ì‚¬í•­:
- Google News RSS â†’ ë„¤ì´ë²„ ë‰´ìŠ¤ API ì „í™˜
- LLM ê¸°ë°˜ ìë™ ë¶„ë¥˜ ë° ìš”ì•½
- ì¡°ëŒ€í‘œë‹˜ ë§ì¶¤ í‚¤ì›Œë“œ ì²´ê³„ ì ìš©
"""

import json
import logging
import os
import re
import requests
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from mvp_config import APIConfig, BusinessKeywords, ProcessingConfig, QualityConfig

# Windows ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/naver_news_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class NaverNewsCollector:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.client_id = APIConfig.NAVER_NEWS_CLIENT_ID
        self.client_secret = APIConfig.NAVER_NEWS_CLIENT_SECRET
        self.api_url = ProcessingConfig.NAVER_NEWS_API_URL
        self.headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret
        }
        
        # ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì €ì¥
        self.collected_news = []
        
    def safe_encode_text(self, text: str) -> str:
        """ì¸ì½”ë”© ì•ˆì „ì„± ë³´ì¥ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
        if not text:
            return ""
        
        try:
            # UTF-8 ì•ˆì „ ì²˜ë¦¬
            text = str(text).encode('utf-8', errors='ignore').decode('utf-8')
            
            # HTML íƒœê·¸ ì œê±°
            text = re.sub(r'<[^>]+>', '', text)
            
            # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
            text = re.sub(r'&[a-zA-Z0-9#]+;', '', text)  # HTML ì—”í‹°í‹°
            text = re.sub(r'\s+', ' ', text)              # ì—°ì† ê³µë°±
            
            return text.strip()
            
        except Exception as e:
            logging.warning(f"[ENCODING] í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
            return "í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜"
    
    def search_news_by_keyword(self, keyword: str, display: int = None) -> List[Dict]:
        """í‚¤ì›Œë“œë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        display = display or ProcessingConfig.NEWS_DISPLAY_COUNT
        
        params = {
            'query': keyword,
            'display': min(display, 100),  # ë„¤ì´ë²„ API ìµœëŒ€ 100ê°œ
            'start': 1,
            'sort': ProcessingConfig.NEWS_SORT_OPTION
        }
        
        try:
            response = requests.get(self.api_url, headers=self.headers, params=params)
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            for item in data.get('items', []):
                article = {
                    'title': self.safe_encode_text(item.get('title', '')),
                    'link': item.get('link', ''),
                    'description': self.safe_encode_text(item.get('description', '')),
                    'pubDate': item.get('pubDate', ''),
                    'keyword': keyword
                }
                
                # í’ˆì§ˆ í•„í„°ë§
                if self.is_quality_article(article):
                    articles.append(article)
            
            logging.info(f"[SEARCH] '{keyword}' í‚¤ì›Œë“œ: {len(articles)}ê±´ ìˆ˜ì§‘")
            return articles
            
        except requests.exceptions.RequestException as e:
            logging.error(f"[ERROR] '{keyword}' ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
        except Exception as e:
            logging.error(f"[ERROR] '{keyword}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []
    
    def is_quality_article(self, article: Dict) -> bool:
        """ê¸°ì‚¬ í’ˆì§ˆ ê²€ì‚¬"""
        title = article.get('title', '')
        description = article.get('description', '')
        
        # ìµœì†Œ ê¸¸ì´ ê²€ì‚¬
        if len(title) < QualityConfig.MIN_TITLE_LENGTH:
            return False
        
        if len(description) < QualityConfig.MIN_CONTENT_LENGTH:
            return False
        
        # ì œì™¸ í‚¤ì›Œë“œ ê²€ì‚¬
        content = f"{title} {description}".lower()
        for exclude_word in QualityConfig.EXCLUDE_KEYWORDS:
            if exclude_word in content:
                return False
        
        return True
    
    def parse_naver_date(self, date_string: str) -> str:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ë‚ ì§œ í˜•ì‹ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # ë„¤ì´ë²„ API ë‚ ì§œ í˜•ì‹: "Mon, 11 Dec 2023 10:30:00 +0900"
            dt = datetime.strptime(date_string, "%a, %d %b %Y %H:%M:%S %z")
            return dt.strftime("%Y-%m-%d")
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ
            return datetime.now().strftime("%Y-%m-%d")
    
    def collect_all_categories(self) -> List[Dict]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []
        categories = BusinessKeywords.get_all_categories()
        
        logging.info(f"[START] {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        for category, keywords in categories.items():
            logging.info(f"[CATEGORY] {category} ìˆ˜ì§‘ ì¤‘...")
            
            category_articles = []
            for keyword in keywords:
                articles = self.search_news_by_keyword(
                    keyword, 
                    ProcessingConfig.MAX_ARTICLES_PER_KEYWORD
                )
                
                # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                for article in articles:
                    article['category'] = category
                    article['formatted_date'] = self.parse_naver_date(article['pubDate'])
                
                category_articles.extend(articles)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì¤‘ë³µ ì œê±° ë° ìµœì‹ ìˆœ ì •ë ¬
            unique_articles = self.remove_duplicates(category_articles)
            unique_articles.sort(key=lambda x: x['pubDate'], reverse=True)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ 10ê°œë¡œ ì œí•œ
            selected_articles = unique_articles[:10]
            all_articles.extend(selected_articles)
            
            logging.info(f"[CATEGORY] {category}: {len(selected_articles)}ê±´ ì„ ë³„ ì™„ë£Œ")
        
        logging.info(f"[COMPLETE] ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_articles)}ê±´")
        return all_articles
    
    def remove_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ê¸°ì‚¬ ì œê±° (ì œëª© ê¸°ì¤€)"""
        seen_titles = set()
        unique_articles = []
        
        for article in articles:
            title = article['title'].lower().strip()
            if title not in seen_titles and len(title) > 5:
                seen_titles.add(title)
                unique_articles.append(article)
        
        return unique_articles
    
    def format_for_notion(self, articles: List[Dict]) -> List[Dict]:
        """ë…¸ì…˜ DB í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜"""
        formatted_articles = []
        
        for article in articles:
            notion_article = {
                "ì œëª©": article['title'],
                "URL": article['link'],
                "ë°œí–‰ì¼": article['formatted_date'],
                "ìš”ì•½": article['description'][:200] + "..." if len(article['description']) > 200 else article['description'],
                "íƒœê·¸": [article['category']],
                "ì¤‘ìš”ë„": "ë³´í†µ",  # LLMì—ì„œ ë‚˜ì¤‘ì— íŒë‹¨
                "ìš”ì•½ í’ˆì§ˆ í‰ê°€": "ë³´í†µ",
                "ì¶œì²˜": "ë„¤ì´ë²„ ë‰´ìŠ¤",
                "í‚¤ì›Œë“œ": article['keyword']
            }
            formatted_articles.append(notion_article)
        
        return formatted_articles
    
    def save_to_json(self, articles: List[Dict], filename: str = "naver_news_data.json"):
        """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            
            logging.info(f"[SAVE] {filename}ì— {len(articles)}ê±´ ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logging.error(f"[ERROR] íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ (MVP1.0)")
    print("=" * 50)
    
    # ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = NaverNewsCollector()
    
    # API í‚¤ í™•ì¸
    if collector.client_id == "YOUR_NAVER_NEWS_CLIENT_ID":
        print("âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("mvp_config.pyì—ì„œ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return
    
    # ë‰´ìŠ¤ ìˆ˜ì§‘
    articles = collector.collect_all_categories()
    
    if not articles:
        print("âŒ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë…¸ì…˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    notion_articles = collector.format_for_notion(articles)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    if collector.save_to_json(notion_articles):
        print(f"âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ {len(notion_articles)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"ğŸ“ ì €ì¥ íŒŒì¼: naver_news_data.json")
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        categories = {}
        for article in notion_articles:
            category = article['íƒœê·¸'][0]
            categories[category] = categories.get(category, 0) + 1
        
        print("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ í˜„í™©:")
        for category, count in categories.items():
            emoji = "ğŸ›¡ï¸" if "ë°©ìœ„" in category else "ğŸ”‹" if "ì—ë„ˆì§€" in category else "ğŸ¢"
            print(f"  {emoji} {category}: {count}ê±´")
    else:
        print("âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")

if __name__ == "__main__":
    main() 
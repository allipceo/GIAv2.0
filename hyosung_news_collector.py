#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íš¨ì„±ì¤‘ê³µì—… ì „ìš© ë‰´ìŠ¤ ìˆ˜ì§‘ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ (Google News RSS ê¸°ë°˜)
ì‘ì„±ì¼: 2025ë…„ 7ì›” 17ì¼
ì‘ì„±ì: ì„œëŒ€ë¦¬ (Lead Developer)
ëª©ì : íš¨ì„±ì¤‘ê³µì—… ê´€ë ¨ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ë° hyosung_news_data.json ì €ì¥

í˜‘ì—…í—Œì¥ GIA V2.0 ì¤€ìˆ˜:
- ê¸°ì¡´ ê²€ì¦ëœ ì‹œìŠ¤í…œ í™œìš©: google_news_collector.py êµ¬ì¡° ì™„ì „ ì¬í™œìš©
- ìµœì†Œ ê°œë°œ ì›ì¹™: í‚¤ì›Œë“œë§Œ íš¨ì„±ì¤‘ê³µì—… íŠ¹í™”ë¡œ ë³€ê²½
- ì¸ì½”ë”© ì•ˆì „ì„±: Windows CP949 í™˜ê²½ ì™„ì „ í˜¸í™˜
"""

import feedparser
import json
import logging
import os
import re
import sys
from datetime import datetime
from urllib.parse import quote

# Windows ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ ë°©ì§€ ì„¤ì •
if sys.platform.startswith('win'):
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.detach())
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.detach())

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('hyosung_news_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# íš¨ì„±ì¤‘ê³µì—… ê´€ë ¨ í‚¤ì›Œë“œ (ê¸°ì—… ì¡°ì‚¬ ë° ë³´í—˜ ì˜ì—… ìµœì í™”)
KEYWORDS = {
    "íšŒì‚¬ëª…": ["íš¨ì„±ì¤‘ê³µì—…", "íš¨ì„±", "Hyosung Heavy Industries"],
    "ì „ë ¥ê¸°ê¸°": ["ì „ë ¥ê¸°ê¸°", "ë³€ì••ê¸°", "ì°¨ë‹¨ê¸°", "ì´ˆê³ ì••ë³€ì••ê¸°", "HVDC", "ìŠ¤ë§ˆíŠ¸ë³€ì „ì†Œ"],
    "ì‹ ì¬ìƒì—ë„ˆì§€": ["ESS", "ì—ë„ˆì§€ì €ì¥ì‹œìŠ¤í…œ", "í’ë ¥ë°œì „", "íƒœì–‘ê´‘", "ì‹ ì¬ìƒì—ë„ˆì§€"],
    "í•´ì™¸ì‚¬ì—…": ["ë©¤í”¼ìŠ¤", "ì°½ì›", "í•´ì™¸ìˆ˜ì£¼", "ê¸€ë¡œë²Œ", "ë¶ë¯¸", "ì¤‘ë™", "ìœ ëŸ½"],
    "ê±´ì„¤ì‚¬ì—…": ["í•´ë§í„´í”Œë ˆì´ìŠ¤", "ë°ì´í„°ì„¼í„°", "í´ë¦°ë£¸", "EPC", "í”ŒëœíŠ¸"]
}

# Google News RSS ê¸°ë³¸ URL (í•œêµ­ì–´, í•œêµ­ ì§€ì—­)
GOOGLE_NEWS_RSS_BASE = "https://news.google.com/rss/search"
RSS_PARAMS = "hl=ko&gl=KR&ceid=KR:ko"

# ì„¤ì •ê°’
MAX_ARTICLES_PER_KEYWORD = 5  # í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ (íš¨ì„±ì¤‘ê³µì—… ì „ìš©ìœ¼ë¡œ ì¦ê°€)
NEWS_DATA_FILE = "hyosung_news_data.json"

def safe_encode_text(text):
    """
    ì¸ì½”ë”© ì•ˆì „ì„±ì„ ë³´ì¥í•˜ëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
    ëª¨ë“  íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨ì§€, ì™¸êµ­ì–´ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    """
    if not text:
        return ""
    
    try:
        # 1ë‹¨ê³„: ë¬¸ìì—´ë¡œ ë³€í™˜
        text = str(text)
        
        # 2ë‹¨ê³„: UTF-8ë¡œ ì¸ì½”ë”© í›„ ì—ëŸ¬ ë¬¸ì ì œê±°
        text = text.encode('utf-8', errors='ignore').decode('utf-8')
        
        # 3ë‹¨ê³„: CP949ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” ë¬¸ìë“¤ ì œê±°/ëŒ€ì²´
        # ì´ëª¨ì§€ ì œê±° (U+1F000-U+1F9FF ë²”ìœ„)
        text = re.sub(r'[\U0001F000-\U0001F9FF]', '', text)
        
        # ê¸°íƒ€ íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì œê±°
        text = re.sub(r'[\u2000-\u206F\u2E00-\u2E7F\u3000-\u303F]', '', text)
        
        # ì œì–´ ë¬¸ì ì œê±°
        text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
        
        # 4ë‹¨ê³„: CP949 í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
        try:
            text.encode('cp949')
        except UnicodeEncodeError:
            # CP949ë¡œ ì¸ì½”ë”© ë¶ˆê°€ëŠ¥í•œ ë¬¸ìë“¤ì„ ë¹„ìŠ·í•œ ë¬¸ìë¡œ ëŒ€ì²´
            text = text.replace('â€”', '-')
            text = text.replace('â€“', '-')
            text = text.replace('"', '"')
            text = text.replace('"', '"')
            text = text.replace(''', "'")
            text = text.replace(''', "'")
            text = text.replace('â€¦', '...')
            
        return text.strip()
    
    except Exception as e:
        logging.error(f"í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return str(text)[:100]  # ì˜¤ë¥˜ ì‹œ ì• 100ìë§Œ ë°˜í™˜

def safe_print(text):
    """ì•ˆì „í•œ ì¶œë ¥ í•¨ìˆ˜ - Windows CP949 ì¸ì½”ë”© ë¬¸ì œ ë°©ì§€"""
    try:
        safe_text = safe_encode_text(text)
        print(safe_text)
    except Exception as e:
        print(f"[PRINT_ERROR] ì¶œë ¥ ì˜¤ë¥˜: {str(e)}")

def clean_html_tags(text):
    """HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    if not text:
        return ""
    
    # ì•ˆì „í•œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    text = safe_encode_text(text)
    
    # HTML íƒœê·¸ ì œê±°
    clean_text = re.sub(r'<[^>]+>', '', text)
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    clean_text = re.sub(r'\s+', ' ', clean_text)
    # ì•ë’¤ ê³µë°± ì œê±°
    return clean_text.strip()

def format_korean_date(date_string):
    """RSS í”¼ë“œì˜ ë‚ ì§œë¥¼ í•œêµ­ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        # feedparserê°€ íŒŒì‹±í•œ ë‚ ì§œ êµ¬ì¡°ì²´ ì²˜ë¦¬
        if hasattr(date_string, 'tm_year'):
            dt = datetime(*date_string[:6])
        else:
            # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
            dt = datetime.strptime(date_string, "%a, %d %b %Y %H:%M:%S %Z")
        
        return dt.strftime("%Y-%m-%d")
    except:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì˜¤ëŠ˜ ë‚ ì§œ ë°˜í™˜
        return datetime.now().strftime("%Y-%m-%d")

def determine_importance(title, category):
    """ê¸°ì‚¬ ì œëª©ê³¼ ì¹´í…Œê³ ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¤‘ìš”ë„ íŒë‹¨ (íš¨ì„±ì¤‘ê³µì—… íŠ¹í™”)"""
    high_keywords = ["íš¨ì„±ì¤‘ê³µì—…", "ëŒ€ê·œëª¨", "ìˆ˜ì£¼", "íˆ¬ì", "í™•ëŒ€", "í•´ì™¸", "ì‹ ê¸°ë¡", "í˜ì‹ ", "íšê¸°ì ", "ìµœì´ˆ"]
    medium_keywords = ["íš¨ì„±", "ì „ë ¥ê¸°ê¸°", "ë³€ì••ê¸°", "ESS", "ì„±ì¥", "ê°œë°œ", "ì‹œì¥", "ë™í–¥", "ë©¤í”¼ìŠ¤", "ì°½ì›"]
    
    title_lower = title.lower()
    
    # ê³ ì¤‘ìš”ë„ í‚¤ì›Œë“œ í¬í•¨ ì‹œ
    if any(keyword in title for keyword in high_keywords):
        return "ë†’ìŒ"
    # ì¤‘ê°„ì¤‘ìš”ë„ í‚¤ì›Œë“œ í¬í•¨ ì‹œ
    elif any(keyword in title for keyword in medium_keywords):
        return "ì¤‘ê°„"
    else:
        return "ë³´í†µ"

def collect_google_news_rss(keywords_dict):
    """Google News RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    all_articles = []
    
    for category, keywords in keywords_dict.items():
        safe_print(f"\n=== {category} ì¹´í…Œê³ ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ ===")
        
        for keyword in keywords:
            safe_print(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
            
            # URL ì¸ì½”ë”©ëœ ì¿¼ë¦¬ ìƒì„±
            encoded_keyword = quote(keyword)
            search_url = f"{GOOGLE_NEWS_RSS_BASE}?q={encoded_keyword}&{RSS_PARAMS}"
            
            try:
                # RSS í”¼ë“œ íŒŒì‹±
                feed = feedparser.parse(search_url)
                
                if feed.bozo:
                    logging.warning(f"RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {keyword}")
                
                # ê¸°ì‚¬ ìˆ˜ì§‘
                articles_count = 0
                for entry in feed.entries:
                    if articles_count >= MAX_ARTICLES_PER_KEYWORD:
                        break
                    
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    title = clean_html_tags(entry.title)
                    link = entry.link
                    description = clean_html_tags(entry.get('description', ''))
                    published = format_korean_date(entry.published_parsed)
                    
                    # ì¤‘ìš”ë„ íŒë‹¨
                    importance = determine_importance(title, category)
                    
                    # ê¸°ì‚¬ ê°ì²´ ìƒì„±
                    article = {
                        "ì œëª©": title,
                        "URL": link,
                        "ìš”ì•½": description,
                        "ë°œí–‰ì¼": published,
                        "ì¹´í…Œê³ ë¦¬": category,
                        "í‚¤ì›Œë“œ": keyword,
                        "ì¤‘ìš”ë„": importance,
                        "ìˆ˜ì§‘ì¼ì‹œ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    
                    all_articles.append(article)
                    articles_count += 1
                    
                    safe_print(f"  âœ“ [{importance}] {title}")
                
                safe_print(f"í‚¤ì›Œë“œ '{keyword}': {articles_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                
            except Exception as e:
                logging.error(f"í‚¤ì›Œë“œ '{keyword}' ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                safe_print(f"  âœ— ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return all_articles

def save_to_json(articles, filename):
    """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        existing_data = []
        if os.path.exists(filename):
            with open(filename, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ê¸°ì¡´ ì œëª© ì„¸íŠ¸
        existing_titles = {article.get('ì œëª©', '') for article in existing_data}
        
        # ìƒˆë¡œìš´ ê¸°ì‚¬ë§Œ ì¶”ê°€
        new_articles = []
        for article in articles:
            if article['ì œëª©'] not in existing_titles:
                new_articles.append(article)
        
        # ì „ì²´ ë°ì´í„° = ê¸°ì¡´ + ìƒˆë¡œìš´
        all_data = existing_data + new_articles
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        safe_print(f"\nâœ“ ì´ {len(all_data)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ (ì‹ ê·œ: {len(new_articles)}ê°œ)")
        safe_print(f"ì €ì¥ ìœ„ì¹˜: {filename}")
        
        return len(new_articles)
        
    except Exception as e:
        logging.error(f"JSON ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        safe_print(f"âœ— ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        return 0

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    safe_print("=" * 60)
    safe_print("ğŸ­ íš¨ì„±ì¤‘ê³µì—… ì „ìš© ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì‹œì‘")
    safe_print("=" * 60)
    
    start_time = datetime.now()
    
    try:
        # ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
        articles = collect_google_news_rss(KEYWORDS)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        new_count = save_to_json(articles, NEWS_DATA_FILE)
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        safe_print("\n" + "=" * 60)
        safe_print("ğŸ“Š íš¨ì„±ì¤‘ê³µì—… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        safe_print("=" * 60)
        safe_print(f"ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        safe_print(f"ì‹ ê·œ ê¸°ì‚¬ ìˆ˜: {new_count}ê°œ")
        safe_print(f"ì†Œìš” ì‹œê°„: {duration.seconds}ì´ˆ")
        safe_print(f"ì™„ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        logging.info(f"íš¨ì„±ì¤‘ê³µì—… ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ ìˆ˜ì§‘, {new_count}ê°œ ì‹ ê·œ")
        
    except Exception as e:
        logging.error(f"ë©”ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        safe_print(f"âœ— í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")

if __name__ == "__main__":
    main() 